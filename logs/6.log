==================================================
device = cuda
device_id = 2
prev = 0
times = 1
global_rounds = 50
eval_gap = 1
batch_size = 10
local_epochs = 1
local_learning_rate = 3e-05
learning_rate_decay = True
learning_rate_decay_gamma = 0.99
auto_break = False
top_cnt = 100
goal = test
save_folder_name = items
model = CLIPWithAdapter
num_classes = 6
algorithm = FedAvg
dataset = DRnpz
data_root = /data/yuxiangyang/lhm/FL/data/npz
num_clients = 5
client_drop_rate = 0.0
random_join_ratio = False
join_ratio = 1.0
num_new_clients = 0
fine_tuning_epoch_new = 0
train_slow_rate = 0.0
send_slow_rate = 0.0
time_threthold = 10000
dlg_eval = False
dlg_gap = 100
batch_num_per_client = 2
==================================================

============= Running time: 0th =============
Creating server and clients ...
CLIP model ViT-B/32 loaded
preprocess: Compose(
    Resize(size=224, interpolation=bicubic, max_size=None, antialias=warn)
    CenterCrop(size=(224, 224))
    <function _convert_image_to_rgb at 0x7f043f9dda80>
    ToTensor()
    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))
)
CLIPWithAdapter(
  (clip_model): CLIP(
    (visual): VisionTransformer(
      (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)
      (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (token_embedding): Embedding(49408, 512)
    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (image_adapter1): ImageAdapter(
    (model): Sequential(
      (0): Linear(in_features=768, out_features=192, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=192, out_features=768, bias=True)
    )
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (image_adapter2): ImageAdapter(
    (model): Sequential(
      (0): Linear(in_features=768, out_features=192, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=192, out_features=768, bias=True)
    )
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (text_adapter): TextAdapter(
    (model): Sequential(
      (0): Linear(in_features=512, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=512, bias=True)
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
)

Join ratio / total clients: 1.0 / 5
Finished creating server and clients.

-------------Round number: 0-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 1.8845
Averaged Test Accurancy: 0.0700
Averaged Test AUC: 0.4708
Std Test Accurancy: 0.0343
Std Test AUC: 0.0632
------------------------- time cost ------------------------- 569.4047064781189

-------------Round number: 1-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8984
Averaged Test Accurancy: 0.6861
Averaged Test AUC: 0.9105
Std Test Accurancy: 0.1226
Std Test AUC: 0.0567
------------------------- time cost ------------------------- 709.6968667507172

-------------Round number: 2-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8851
Averaged Test Accurancy: 0.6861
Averaged Test AUC: 0.9110
Std Test Accurancy: 0.1226
Std Test AUC: 0.0567
------------------------- time cost ------------------------- 564.6472280025482

-------------Round number: 3-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8792
Averaged Test Accurancy: 0.6861
Averaged Test AUC: 0.9111
Std Test Accurancy: 0.1226
Std Test AUC: 0.0566
------------------------- time cost ------------------------- 562.6421506404877

-------------Round number: 4-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8763
Averaged Test Accurancy: 0.6861
Averaged Test AUC: 0.9111
Std Test Accurancy: 0.1226
Std Test AUC: 0.0567
------------------------- time cost ------------------------- 552.88916182518

-------------Round number: 5-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8771
Averaged Test Accurancy: 0.6861
Averaged Test AUC: 0.9111
Std Test Accurancy: 0.1226
Std Test AUC: 0.0567
------------------------- time cost ------------------------- 500.3319411277771

-------------Round number: 6-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8765
Averaged Test Accurancy: 0.6861
Averaged Test AUC: 0.9112
Std Test Accurancy: 0.1226
Std Test AUC: 0.0568
------------------------- time cost ------------------------- 537.8314349651337

-------------Round number: 7-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8736
Averaged Test Accurancy: 0.6861
Averaged Test AUC: 0.9112
Std Test Accurancy: 0.1226
Std Test AUC: 0.0569
------------------------- time cost ------------------------- 542.339460849762

-------------Round number: 8-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8732
Averaged Test Accurancy: 0.6861
Averaged Test AUC: 0.9112
Std Test Accurancy: 0.1226
Std Test AUC: 0.0570
------------------------- time cost ------------------------- 514.4228763580322

-------------Round number: 9-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8739
Averaged Test Accurancy: 0.6861
Averaged Test AUC: 0.9112
Std Test Accurancy: 0.1226
Std Test AUC: 0.0570
------------------------- time cost ------------------------- 515.7454993724823

-------------Round number: 10-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8732
Averaged Test Accurancy: 0.6860
Averaged Test AUC: 0.9112
Std Test Accurancy: 0.1226
Std Test AUC: 0.0569
------------------------- time cost ------------------------- 509.0386633872986

-------------Round number: 11-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8725
Averaged Test Accurancy: 0.6860
Averaged Test AUC: 0.9113
Std Test Accurancy: 0.1226
Std Test AUC: 0.0567
------------------------- time cost ------------------------- 504.274311542511

-------------Round number: 12-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8751
Averaged Test Accurancy: 0.6861
Averaged Test AUC: 0.9114
Std Test Accurancy: 0.1226
Std Test AUC: 0.0565
------------------------- time cost ------------------------- 508.1559886932373

-------------Round number: 13-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8724
Averaged Test Accurancy: 0.6860
Averaged Test AUC: 0.9115
Std Test Accurancy: 0.1226
Std Test AUC: 0.0563
------------------------- time cost ------------------------- 513.2246935367584

-------------Round number: 14-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8732
Averaged Test Accurancy: 0.6861
Averaged Test AUC: 0.9115
Std Test Accurancy: 0.1226
Std Test AUC: 0.0561
------------------------- time cost ------------------------- 502.9171941280365

-------------Round number: 15-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8744
Averaged Test Accurancy: 0.6863
Averaged Test AUC: 0.9115
Std Test Accurancy: 0.1226
Std Test AUC: 0.0563
------------------------- time cost ------------------------- 510.99319529533386

-------------Round number: 16-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8786
Averaged Test Accurancy: 0.6861
Averaged Test AUC: 0.9117
Std Test Accurancy: 0.1226
Std Test AUC: 0.0561
------------------------- time cost ------------------------- 506.08736181259155

-------------Round number: 17-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8740
Averaged Test Accurancy: 0.6860
Averaged Test AUC: 0.9118
Std Test Accurancy: 0.1226
Std Test AUC: 0.0561
------------------------- time cost ------------------------- 503.4982326030731

-------------Round number: 18-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8757
Averaged Test Accurancy: 0.6861
Averaged Test AUC: 0.9118
Std Test Accurancy: 0.1226
Std Test AUC: 0.0561
------------------------- time cost ------------------------- 475.72239542007446

-------------Round number: 19-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8722
Averaged Test Accurancy: 0.6860
Averaged Test AUC: 0.9119
Std Test Accurancy: 0.1226
Std Test AUC: 0.0560
------------------------- time cost ------------------------- 494.10659766197205

-------------Round number: 20-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8730
Averaged Test Accurancy: 0.6863
Averaged Test AUC: 0.9119
Std Test Accurancy: 0.1226
Std Test AUC: 0.0560
------------------------- time cost ------------------------- 436.39147329330444

-------------Round number: 21-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8760
Averaged Test Accurancy: 0.6861
Averaged Test AUC: 0.9120
Std Test Accurancy: 0.1226
Std Test AUC: 0.0560
------------------------- time cost ------------------------- 415.994389295578

-------------Round number: 22-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8729
Averaged Test Accurancy: 0.6864
Averaged Test AUC: 0.9120
Std Test Accurancy: 0.1225
Std Test AUC: 0.0559
------------------------- time cost ------------------------- 415.60356187820435

-------------Round number: 23-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8762
Averaged Test Accurancy: 0.6861
Averaged Test AUC: 0.9120
Std Test Accurancy: 0.1226
Std Test AUC: 0.0559
------------------------- time cost ------------------------- 413.2004237174988

-------------Round number: 24-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8745
Averaged Test Accurancy: 0.6860
Averaged Test AUC: 0.9121
Std Test Accurancy: 0.1226
Std Test AUC: 0.0559
------------------------- time cost ------------------------- 421.95203733444214

-------------Round number: 25-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8739
Averaged Test Accurancy: 0.6860
Averaged Test AUC: 0.9121
Std Test Accurancy: 0.1226
Std Test AUC: 0.0559
------------------------- time cost ------------------------- 411.0996935367584

-------------Round number: 26-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8722
Averaged Test Accurancy: 0.6863
Averaged Test AUC: 0.9122
Std Test Accurancy: 0.1226
Std Test AUC: 0.0560
------------------------- time cost ------------------------- 411.171688079834

-------------Round number: 27-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8721
Averaged Test Accurancy: 0.6862
Averaged Test AUC: 0.9123
Std Test Accurancy: 0.1226
Std Test AUC: 0.0561
------------------------- time cost ------------------------- 407.9771590232849

-------------Round number: 28-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8726
Averaged Test Accurancy: 0.6859
Averaged Test AUC: 0.9124
Std Test Accurancy: 0.1225
Std Test AUC: 0.0561
------------------------- time cost ------------------------- 425.43095326423645

-------------Round number: 29-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8716
Averaged Test Accurancy: 0.6863
Averaged Test AUC: 0.9124
Std Test Accurancy: 0.1225
Std Test AUC: 0.0563
------------------------- time cost ------------------------- 420.9222848415375

-------------Round number: 30-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8746
Averaged Test Accurancy: 0.6861
Averaged Test AUC: 0.9125
Std Test Accurancy: 0.1226
Std Test AUC: 0.0562
------------------------- time cost ------------------------- 415.092960357666

-------------Round number: 31-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8723
Averaged Test Accurancy: 0.6864
Averaged Test AUC: 0.9123
Std Test Accurancy: 0.1225
Std Test AUC: 0.0562
------------------------- time cost ------------------------- 405.75686621665955

-------------Round number: 32-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8720
Averaged Test Accurancy: 0.6864
Averaged Test AUC: 0.9124
Std Test Accurancy: 0.1226
Std Test AUC: 0.0562
------------------------- time cost ------------------------- 408.26102900505066

-------------Round number: 33-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8723
Averaged Test Accurancy: 0.6862
Averaged Test AUC: 0.9125
Std Test Accurancy: 0.1226
Std Test AUC: 0.0562
------------------------- time cost ------------------------- 417.00117444992065

-------------Round number: 34-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8715
Averaged Test Accurancy: 0.6863
Averaged Test AUC: 0.9125
Std Test Accurancy: 0.1226
Std Test AUC: 0.0563
------------------------- time cost ------------------------- 418.07865023612976

-------------Round number: 35-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8724
Averaged Test Accurancy: 0.6861
Averaged Test AUC: 0.9125
Std Test Accurancy: 0.1226
Std Test AUC: 0.0562
------------------------- time cost ------------------------- 416.5802562236786

-------------Round number: 36-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8721
Averaged Test Accurancy: 0.6861
Averaged Test AUC: 0.9124
Std Test Accurancy: 0.1226
Std Test AUC: 0.0562
------------------------- time cost ------------------------- 418.4176185131073

-------------Round number: 37-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8712
Averaged Test Accurancy: 0.6863
Averaged Test AUC: 0.9122
Std Test Accurancy: 0.1225
Std Test AUC: 0.0560
------------------------- time cost ------------------------- 415.2075822353363

-------------Round number: 38-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8753
Averaged Test Accurancy: 0.6861
Averaged Test AUC: 0.9125
Std Test Accurancy: 0.1226
Std Test AUC: 0.0560
------------------------- time cost ------------------------- 423.5447120666504

-------------Round number: 39-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8734
Averaged Test Accurancy: 0.6861
Averaged Test AUC: 0.9125
Std Test Accurancy: 0.1226
Std Test AUC: 0.0559
------------------------- time cost ------------------------- 416.9449486732483

-------------Round number: 40-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8702
Averaged Test Accurancy: 0.6862
Averaged Test AUC: 0.9123
Std Test Accurancy: 0.1225
Std Test AUC: 0.0559
------------------------- time cost ------------------------- 429.1446707248688

-------------Round number: 41-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8726
Averaged Test Accurancy: 0.6861
Averaged Test AUC: 0.9124
Std Test Accurancy: 0.1226
Std Test AUC: 0.0559
------------------------- time cost ------------------------- 491.97140884399414

-------------Round number: 42-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8699
Averaged Test Accurancy: 0.6861
Averaged Test AUC: 0.9122
Std Test Accurancy: 0.1226
Std Test AUC: 0.0559
------------------------- time cost ------------------------- 481.4817008972168

-------------Round number: 43-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8700
Averaged Test Accurancy: 0.6861
Averaged Test AUC: 0.9122
Std Test Accurancy: 0.1226
Std Test AUC: 0.0559
------------------------- time cost ------------------------- 491.9203770160675

-------------Round number: 44-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8715
Averaged Test Accurancy: 0.6866
Averaged Test AUC: 0.9116
Std Test Accurancy: 0.1226
Std Test AUC: 0.0557
------------------------- time cost ------------------------- 484.5375897884369

-------------Round number: 45-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8698
Averaged Test Accurancy: 0.6861
Averaged Test AUC: 0.9121
Std Test Accurancy: 0.1226
Std Test AUC: 0.0558
------------------------- time cost ------------------------- 487.8821609020233

-------------Round number: 46-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8699
Averaged Test Accurancy: 0.6862
Averaged Test AUC: 0.9118
Std Test Accurancy: 0.1225
Std Test AUC: 0.0558
------------------------- time cost ------------------------- 490.1450288295746

-------------Round number: 47-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8705
Averaged Test Accurancy: 0.6861
Averaged Test AUC: 0.9121
Std Test Accurancy: 0.1226
Std Test AUC: 0.0559
------------------------- time cost ------------------------- 493.56402230262756

-------------Round number: 48-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8718
Averaged Test Accurancy: 0.6861
Averaged Test AUC: 0.9120
Std Test Accurancy: 0.1226
Std Test AUC: 0.0560
------------------------- time cost ------------------------- 482.530161857605

-------------Round number: 49-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8714
Averaged Test Accurancy: 0.6866
Averaged Test AUC: 0.9108
Std Test Accurancy: 0.1226
Std Test AUC: 0.0558
------------------------- time cost ------------------------- 495.7215268611908

-------------Round number: 50-------------

Evaluate global model
Client 0 test data size: 673
Client 1 test data size: 1945
Client 2 test data size: 6884
Client 3 test data size: 88
Client 4 test data size: 240
Averaged Train Loss: 0.8697
Averaged Test Accurancy: 0.6860
Averaged Test AUC: 0.9115
Std Test Accurancy: 0.1225
Std Test AUC: 0.0560
------------------------- time cost ------------------------- 475.6872878074646

Best accuracy.
0.6865717192268566

Average time cost per round.
473.3556130409241
File path: ../results/DRnpz_FedAvg_test_0.h5

Average time cost: 24283.69s.
Length:  51
std for best accurancy: 0.0
mean for best accurancy: 0.6865717192268566
All done!

Storage on cuda:0
-------------------------------------------------------------------------------
Total Tensors: 1076063537 	Used Memory: 2.33G
The allocated memory on cuda:0: 2.39G
Memory differs due to the matrix alignment or invisible gradient buffer tensors
-------------------------------------------------------------------------------

Storage on cpu
-------------------------------------------------------------------------------
Total Tensors: 5929 	Used Memory: 23.50K
-------------------------------------------------------------------------------
